---
title: "Predicting Starbuck's Locations"
author: "Brett"
date: "24 April 2019"
output: word_document
---

# Predicting Starbuck's Locations
### Brett Waugh
### 24 April 2019
### capstone.R
### Different analytic techniques to predict the number of Starbuck's in an area. 
### Data was retrieved from: https://www.kaggle.com/starbucks/store-locations


Predicting the number of Starbucks locations has many practical uses and interested groups. Starbucks is interested in predicting where they should put a new franchise and competing companies may want this information in order to prepare for the new competition. This study walks through six different methods for predicting Starbucks locations. The first three methods use data Starbucks provided and show methods that everyday individuals may use to predict Starbucks locations. The next three methods will use data from several sources and more advanced modeling techniques to predict Starbucks locations. The benefits and costs of each method are discussed; exploring if using more complex modeling is warranted over other methods.  
```{r libraries, include=FALSE}
# Necessary libraries.
require(readr)
require(ggplot2)
require(dplyr)
require(stats)
require(plyr)
require(cluster)
require(lattice)
require(graphics)
require(grid)
require(gridExtra)
require(mgcv)
require(caret)
require(e1071)
require(boot)
require(ISLR)
require(arm)

# Load in the data. Data was provided by: https://www.kaggle.com/starbucks/store-locations
directory <- read_csv("~/Documents/IDS4934/capstone/directory.csv")

# States and counts calculated in Splunk. 
starbucksByState <- read_csv("~/Documents/IDS4934/capstone/starbucksByState.csv")

# Make into a dataframe.
df <- as.data.frame(directory)

# Set seed for study.
set.seed(950)
```

## The original dataset
The original dataset was found on Kaggle (Starbucks, 2017). There were some datasets online that required a membership or cost to use, but the Kaggle dataset was from of cost. There were many fields in this dataset that were not needed for this project and were stripped away early on to expedite processing.  

The scope of the project included all Starbucks located in the United States, regardless or ownership type. These distinctions were important to validate because the original dataset provided both Starbucks and Teavana because Starbucks now owns Teavana. The original dataset also provided the locations for all Starbucks in the world, so this needed to be trimmed down to only United States locations. With these altercations made to the dataset, the size of the file shrunk from about 10 MB to around 1.5 MB. 

From the original dataset, an additional file was created for the number of Starbucks locations by state. This file is very small and allowed some calculations to be done much quicker. 
```{r dataFiltering, include=FALSE}
# Only keep stores that are Starbucks. Teavana stores were originally included. 
df1 <- subset(df, Brand=="Starbucks")

# Verify that only Starbucks stores remain. 
nrow(directory[directory$Brand!="Starbucks",])

# Drop rows that do not matter to the analysis. 
df2 <- subset(df1, select = -c(`Store Number`,`Store Number`, `Store Name`, `Ownership Type`, `Street Address`, `Phone Number`, `Timezone`))

# Verify that those fields were dropped.
str(df2)

# Only keep stores that are in the United States. Dataset provided worldwide locations. 
df3 <- subset(df2, Country=="US")

# Verify that only US locations were kept. 
nrow(df3[df3$Country!="US",])
```

## Method One: Mean
There are many techniques commonly used for predicting values. Many of the most commonly used ones are used not because they achieve more accurate results, but because they provide a quick way to get a feel for the data. Averaging is one of these techniques that may not produce the best representation of the dataset, but it is popular and quick to implement.  

Using the file with the associated states and number of Starbucks in that state, the average is quickly worked out to 261 Starbucks locations per state. 
```{r firstMethod, include=TRUE}
# Average number of Starbucks across the US.
meanStarbucks <- nrow(df3)/nrow(starbucksByState)
```
```{r, include=TRUE}
# Histogram of "Number of Starbucks by State". G1.
ggplot(df3, aes(x=df3$`State/Province`, col=df3$`State/Province`, fill=df3$`State/Province`)) + 
  geom_histogram(stat="count") +  
  theme(axis.text.x = element_text(angle = -90), legend.position = "none") + 
  labs(x = "State", y = "Number of Starbucks", title = "Number of Starbucks by State", caption = "based on data from: https://www.kaggle.com/starbucks/store-locations")

# Histogram of "Number of Starbucks by State" with mean line. G2. 
ggplot(df3, aes(x=df3$`State/Province`, col=df3$`State/Province`, fill=df3$`State/Province`)) + 
  geom_histogram(stat="count") +  
  theme(axis.text.x = element_text(angle = -90), legend.position = "none") + 
  labs(x = "State", y = "Number of Starbucks", title = "Number of Starbucks by State", caption = "based on data from: https://www.kaggle.com/starbucks/store-locations") +
  geom_hline(yintercept=meanStarbucks)
```
```{r, include=FALSE}
# Accuracy for first method. 
# No states match this mean! Closest results are: MA has 262, MD has 252, MI has 272, NV has 249, and NJ has 249. 
method1Result <- nrow(starbucksByState[starbucksByState$count==meanStarbucks,])/nrow(starbucksByState)
method1Result
```
After viewing the histogram, we can see that this number does not encapsulate a single state’s number of Starbucks locations. There are a few states that come close (MA has 262, MD has 252, MI has 272, NV has 249, and NJ has 249) but none that are this exact number. Averaging can be useful for getting a quick feel for a dataset, but it does not accurately represent the distribution of the dataset.  

## Method Two: Within a Standard Deviation, Removing Outliers 
To improve upon the mistakes from Method 1, a deeper understanding into the distribution of the data is needed.  
```{r secondMethodBoxplot, include=TRUE}
# Bowplot to determine outliers. G4.
ggplot(starbucksByState, aes(y=starbucksByState$count)) + 
  geom_boxplot(outlier.colour = "red", show.legend = NA, outlier.shape = 8, outlier.size = 5) +  
  theme(legend.position = "none") + 
  labs(y = "Number of Starbucks", title = "Distribution of Starbucks in United States", caption = "based on data from: https://www.kaggle.com/starbucks/store-locations")
```
As shown in the boxplot, there are three upper bound outliers. The largest one (CA) being so much more than the rest that it heavily distorts the graph. The standard deviation before removing any outliers is 421.41, but by removing the three upper bound outliers the standard deviation drops drastically to 173.17.
```{r secondMethod, include=FALSE}
# Standard deviation.
allSD <- sd(starbucksByState$count)
allSD

# Data without outliers: CA, TX, WA is 173.17. 
sb1 <- subset(starbucksByState, State_Province!="CA" & State_Province!="TX" & State_Province!="WA")
sb1

# Mean of data without outliers. 
meanSB1 <- mean(sb1$count)
meanSB1

# Standard deviation of data without outliers. 
sdSB1 <- sd(sb1$count)
sdSB1

# Upper and lower limits.
upSB1 <- meanSB1 + sdSB1
upSB1

loSB1 <- meanSB1 - sdSB1
loSB1

# Histogram of "Number of Starbucks by State" within one standard deviation (93.05,439.39). G5.
ggplot(sb1, aes(x=sb1$State_Province, y=sb1$count, col=sb1$State_Province, fill=sb1$State_Province)) + 
  geom_col() +  
  theme(axis.text.x = element_text(angle = -90), legend.position = "none") + 
  labs(x = "State", y = "Number of Starbucks", title = "Number of Starbucks by State", caption = "based on data from: https://www.kaggle.com/starbucks/store-locations") +
  geom_hline(yintercept = upSB1) +
  geom_hline(yintercept = loSB1)

# Boxplot for "Distribution of Starbucks in United States without CA, TX, and WA". G6.
ggplot(sb1, aes(y=sb1$count)) + 
  geom_boxplot(outlier.colour = "red", show.legend = NA, outlier.shape = 8, outlier.size = 5) +  
  theme(legend.position = "none") + 
  labs(y = "Number of Starbucks", title = "Distribution of Starbucks in United States without CA, TX, WA", caption = "based on data from: https://www.kaggle.com/starbucks/store-locations")


# Accuracy for second method. 
method2Result <- nrow(sb1[sb1$count<=upSB1 & sb1$count>=loSB1,])/nrow(starbucksByState)
method2Result
```
Notice that by removing the three upper bound outliers and staying within one standard deviation, much more of the state’s Starbucks locations are encapsulated. This method produced an upper and lower bound of [9.29, 355.62] which included 76.47% of the states included. This is a great method if all of the data is readily available but does not discriminate with any features of a particular state. A part of the success to this method is the drastic range in the bounds, a range of 346.33. 

Notice that by removing the three upper bound outliers and staying within one standard deviation, much more of the state’s Starbuck’s locations are encapsulated. This method produced an upper and lower bound of [9.29, 355.62] which included 76.47% of the states included. This is a great method if all of the data is readily available but does not discriminate with any features of a particular state. A part of the success to this method is the drastic range in the bounds, a range of 346.33. 

## Method Three: Linear Regression
Linear regression is a useful technique in finding patterns in data. The underlying principle is attempting to create a line that includes the greatest number of points (Zhao, 2013). In the current dataset, the latitude and longitude are the only numerical values that linear regression can be used on.  

```{r thirdMethod, include=TRUE}
# Map showing "Linear Regression on Starbucks locations". G9.
ggplot(df3, aes(x=Longitude, y=Latitude)) + 
  geom_smooth(method="lm") + 
  geom_point() +  
  labs(x="Longitude", y = "Latitude", title = "Linear Regression on Starbucks locations", caption = "based on data from: https://www.kaggle.com/starbucks/store-locations")

# Correlation between latitude and longitude.
cor(df3$Longitude, df3$Latitude)

# Creating the linear model. 
linearMod <- lm(df3$Longitude ~ df3$Latitude, data=df3)
summary(linearMod)
```
The plot above shows every United States Starbucks location. Linear regression does not fit the data well in this case, the correlation was –0.003 with an adjust R-squared value of –6.82 x 10^-5. The failure in using linear regression to find a pattern in the placement of Starbucks shows that Starbucks locations depend on more features than just latitude and longitude for placement.  

### The need for more data
Working only with the data provided in the original dataset, it is clear that more information is needed to determine how Starbucks places its locations. Additional data was collected from several locations and combined to provide additional information on state’s: population (Enchanted Learning, & U.S. Census Bureau, 2017), median income (U.S. Census Bureau, & Wikipedia,2018), number of universities (National Center for Education Statistics, & U.S. Department of Education, 2013), and crime rate per 100,000 people (Johnson, 2016). With these additional features, better predictions may be made regarding locating Starbucks.     

```{r moreData, include=FALSE}
# Used Splunk to combine files into a single file. Sources for the file are from several different
# locations. 
addData <- read_csv("~/Documents/IDS4934/capstone/addData.csv")
```

## Method Four: Multivariate Linear Regression
Similar to the fourth method, multivariate linear regression is trying to match variables to an outcome variable (Zhao, 2013). With multivariate linear regression, many more features are used. This method is particularly useful in determining useful features for other models, because it is so easy to setup and processes quickly.  
```{r fourthMethod, include=FALSE}
# First run to see which features are relevant. 
linFit <- lm(addData$starbucks ~ addData$income + addData$numUni + addData$population + addData$crime, data=addData)
summary(linFit)

# Determine the RMSE of the linear model. 
linFitRMSE <- sqrt(mean(linFit$residuals^2))
linFitRMSE

# First run to see which features are relevant. 
linFit2 <- lm(addData$starbucks ~ addData$income + addData$numUni + addData$population, data=addData)
summary(linFit2)

# Determine the RMSE of the linear model. 
linFit2RMSE <- sqrt(mean(linFit2$residuals^2))
linFit2RMSE
```
For the first linear model, the outcome variable is the number of Starbucks and the other variables are: population, median income, number of Universities, and crime rate. After running the model, the results indicate that only population, median income, and number of Universities are significant features (P Value < 0.05) to the model. The first model has an RMSE of 164.62 and an Adjusted R sqaured value of 0.83. For the second model, the crime variable was removed and the RMSE increased slightly to 165.00 while the Ajusted R squared stayed at 0.83.  

The crime variable made little difference in the performance of the models. The three features with the most significance are population, median income, and number of Universities. The RMSE of around 165 is a reasonable value considering the dataset. The models were able to be setup quickly and easily, with little hassle involved.  

## Method Five: Logistic Regression
The model often used for logistic regression is the Generalized Linear Model (GLM) (Dietrich et al., 2015). This model shares the fourth method’s ease of use and can be quickly setup. 
```{r fifthMethod, include=FALSE}
# First logistic model including all fields in additional data. 
logmod1 <- glm(addData$starbucks ~ addData$income + addData$numUni + addData$population + addData$crime, data=addData)
summary(logmod1) # display results

# Calculate RMSE for the first logistic model.
logmod1RMSE <- sqrt(mean(logmod1$residuals^2))
logmod1RMSE

# Second logistic model, excluding crime rate from the additional data. 
logmod2 <- glm(starbucks ~ income + numUni + population, data=addData)
summary(logmod2)

# Calculate RMSE for the second logistic model. 
logmod2RMSE <- sqrt(mean(logmod2$residuals^2))
logmod2RMSE

# Third logistic model (Bayesian Generalized Linear Model), excludes crime rate from additional data.
# Also performs ten fold cross validation on data. 
ControlParameters <-trainControl(method="repeatedcv", 
                                 number=10,
                                 repeats=10)

logmod3 <-train(starbucks ~ income + numUni + population, 
                   data=addData,
                   method='bayesglm',
                   trControl= ControlParameters
)
logmod3
logmod3RMSE <- 201.2931
```
The first logistic model is setup with all the features: population, median income, number of Universities, and crime. This model received an RMSE of 164.62. This model performed about the same as the models from the fourth method. For the second logistic model, the crime feature is removed and the RMSE increases to 165.00, similar to what happened in the fifth method.  

The third model was constructed using a Bayesian GLM instead of the same method as the previous two. This model took more effort to setup and was performed with ten-fold cross validation. The RMSE for this model was 201.29 with an R squared of 0.80. 

The logistic models performed about the same as the linear models, save for the Bayesian GLM that increased the RMSE by about 40. The first two models were simply constructed while the third model took more understanding about model creation to perform.  

## Method Six: Support Vector Machine
The Support Vector Machine (SVM) models are considered one of the best out of the box classifiers (Witten et al., 2017). This method handles multiple variables well but takes more knowledge to setup and can take more computational power (Zaki & Meira , 2014; Welling, 2010).
```{r sixthMethod, include=FALSE}
# Put data into a dataframe. 
df4 <- as.data.frame(addData[2:5])

# Create a training and testing set. 
svm_size <- round(.8 * dim(df4)[1])
svm_train <- df4[1:svm_size,]
svm_test <- df4[-(1:svm_size),]

# First SVM model with C=1, and ten fold cross validation. 
svmMod = svm(df4$starbucks ~ df4$income + df4$numUni + df4$population, data = df4, cost=10, epsilon=1, cross=10, scale=F, kernal='radial')
print(svmMod)

# Results of training set. 
svmModPred <- predict(svmMod, svm_train)
errval <- df4$starbucks - svmModPred
svm_RMSE <- RMSE(errval, obs=df4$starbucks)
print(paste('SVM RMSE: ', svm_RMSE))

# Grid search for best Epsilon and Cost values. Made with help from: https://rpubs.com/richkt/280840
tuneResult1 <- tune(svm, df4$starbucks ~ df4$income + df4$numUni + df4$population,  data = df4,
                    ranges = list(epsilon = seq(0,1,0.01), cost = seq(0.01,5,0.05))
)

# Map tuning results
plot(tuneResult1)

# Continuation of Grid Search.
tuneResult <- tune(svm, df4$starbucks ~ df4$income + df4$numUni + df4$population,  data = df4,
                   ranges = list(epsilon = seq(tuneResult1$best.model$epsilon*1.01,
                                               tuneResult1$best.model$epsilon*1.1,
                                               length.out = 10), 
                                 cost = seq(tuneResult1$best.model$cost-1,
                                            tuneResult1$best.model$cost+1,
                                            length=10)))

plot(tuneResult)
print(tuneResult)

# Final SVM model with values from tuneResult.
svmModTuned = svm(df4$starbucks ~ df4$income + df4$numUni + df4$population, data = df4, cost=tuneResult$best.parameters$cost, epsilon=tuneResult$best.parameters$epsilon, cross=10, scale=F, kernal='radial')
print(svmModTuned)

# Results of training set. 
svmModPredTuned <- predict(svmModTuned, svm_train)
errvalTuned <- df4$starbucks - svmModPredTuned
svm_RMSETuned <- RMSE(errvalTuned, obs=df4$starbucks)
print(paste('Tuned SVM RMSE: ', svm_RMSETuned))
```
The first SVM was trained with the features: population, median income, and crime rate. It also used a Cost of 10, Epsilon of 1, and ten-fold cross validation. This model was able to produce RMSE of 121.06, significantly less than the RMSE of the previous methods. 

Because SVMs have more variables to test than just the features put into it, grid search was used to select the best Cost and Epsilon variables for the second SVM model (Rich, 2017). Grid search saves time because the user does not have to manually test each value when creating the SVM. Using grid search is computationally expensive and can take a while to perform.  

Using the Cost and Epsilon values from the grid search, along with the same features of the other model, and ten-fold cross validation produced an RMSE of 119.23. This is the lowest RMSE so far but costed the most computational power and required significant background knowledge to perform.  

## Results from the Models
A variety of models were used in methods four, five, and six. Each of these models took different amounts of background knowledge, computational resources, and time to setup correctly. The linear models in method four took the shortest amount of time to setup, while the SVMs in method six took the most amount of time to setup. The SVMs also took the most computational resources because of the grid search. The comparison of the RMSE results is shown below.
```{r, include=TRUE}
# Create a dataframe for the model results. 
resultsDF <- data.frame(model=c(linFitRMSE, 
                                   linFit2RMSE, 
                                   logmod1RMSE, 
                                   logmod2RMSE, 
                                   logmod3RMSE, 
                                   svm_RMSE, 
                                   svm_RMSETuned), 
                        seq=1:7,
                        names=c("Linear Model (all features)", 
                                "Linear Model (without crime)", 
                                "GLM (all features)", 
                                "GLM (without crime)", 
                                "Bayesian GLM (without crime)",
                                "SVM (without crime)",
                                "SVM (without crime, tuned)"))

# Graph of models and RMSE. 
ggplot(resultsDF, aes(names, model, col=names)) + 
  geom_point(size=6) +
  labs(x="Model Number", y = "RMSE", title = "Model Performance") +
  theme(axis.text.x = element_text(angle = -45))
```
The best performing model ended up being the final SVM that was tuned using grid search. The other SVM had a comparable score using standard default values for the Cost and Epsilon. The logistic and linear models performed almost identically, save for the Bayesian GLM which produced a significantly higher RMSE than the other models.   

## Conclusion
There are numerous methods to help people predict the number of Starbucks in an area. The first three methods focused on more traditional methods that people who are not familiar with more advanced techniques would use. These techniques gave people a sense of familiarity with the data but did not provide much depth to the technique. The first three methods also had a very wide range, with little distinction between states. The last three methods built off other features to give more context into the areas that Starbucks tend to appear in. Using these types of techniques, Starbucks location prediction is much more accurate.  

If more granular data was available regarding population, median income, crime rates, and number of Universities by city instead of solely by state then the models may have performed better. The original intent was for the models to perform at the city-level, but a lack of data for all cities forced the models to perform at the state level.  

## References
Dietrich, D., Heller, B., & Yang, B. (2015). Data Science & Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data. Indianapolis, IN: Wiley. 

Downey, A. B. (2015). Think Stats [2.0.38]. Retrieved January 20, 2019. 

Enchanted Learning, & U.S. Census Bureau. (2017, July 1). US States: Population and Ranking. Retrieved April 13, 2019, from https://www.enchantedlearning.com/usa/states/population.shtml 

James, G., Witten, D., Hastie, T., & Tibshirani, R. (2017). An introduction to Statistical Learning: With Applications in R. Retrieved January 20, 2019. 

Johnson, M., Jr. (2016, December 28). United States crime rates by county. Retrieved April 13, 2019, from https://www.kaggle.com/mikejohnsonjr/united-states-crime-rates-by-county 

Matloff, N. (2013). The Art of R programming: A tour of statistical software design. San Francisco, CA: No Starch Press. 

National Center for Education Statistics, & U.S. Department of Education. (2013, August 12). Colleges and Universities in the United States of America (USA) by State/ Possession. Retrieved April 13, 2019, from http://www.univsearch.com/state.php 

Rich, K. T. (2017). Machine learning intro in R: Support Vector Regression. Retrieved April 16, 2019, from https://rpubs.com/richkt/280840 

Starbucks. (2017, February 13). Starbucks Locations Worldwide. Retrieved February, 2019, from https://www.kaggle.com/starbucks/store-locations 

U.S. Census Bureau, & Wikipedia. (2018, December 28). List of U.S. states and territories by income. Retrieved April 13, 2019, from https://en.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_income 

Welling, M. (2010). A First Encounter with Machine Learning. Retrieved January 20, 2019. 

Zaki, M. J., & Meira, W. (2014). Data mining and analysis: Fundamental concepts and algorithms. Retrieved January 20, 2019. 

Zhao, Y. (2013). R and Data Mining: Examples and Case Studies. Retrieved January 20, 2019, from http://www.RDataMining.com  
